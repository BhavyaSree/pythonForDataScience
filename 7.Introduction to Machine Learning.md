## Supervised Learning

Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. 

Supervised learning algorithms are trained using labeled examples such as an input where the desired output is known.

For example, a segment of text could have a category label, like:
* **Spam** vs. **Legitimate** Email
* **Positive** vs. **Negative** Email

### How it works?

The network receives a set of input data along with the corresponding correct outputs and then the algorithm or network will learn by comparing its actual output with correct outputs to find errors.     
Then it will modify the model accordingly such as adjusting the weights and biased values in the network.

* Supervised learning is commonly used in applications where historical data predicts likely future events.

[Presentation on Supervised learning](./Python-DS-Presentations/Intro-to-Machine-Learning.pdf)

Presentation includes Machine Learning Process.  
Train set -- input, 
First, we build a model using training set and this we check the performance of the model using test set.
Test set -- expected output. 

First fit a model on training data and test the model on testing data

## Model Evaluation

The key classification metrics we need to understand are:
* Accuracy
* Recall
* Precision
* F1-score

Accuracy in classification problems is the number of correct predictions made by the model divided by the total number of predictions.  

Recall is the ability of the model is to find all the relevant cases within a dataset.  
Recall is the number of true positive cases divided by number of true positive cases plus number of false negatives.

Precision is the ability of a classification model to identify only the relevant data points.  
Precision is defined as the number of true positives divided by number of true positives plus number of false positives.  

F1-score is the harmonic mean of precision and recall taking both metrics into account in the following equation

F1-score = 2*((precision*recall)/(precision+recall))

We use harmonic mean instead of simple mean because harmonic mean punishes extreme values.  

We can view all correctly classifies results vs incorrectly classified images in the form of an confusion matrix.

For example: 
Model for Diagonsing the presence of a disease through blood sample.   
* True Positive indicates someone having the disease and the model correctly predicting that they have the disease.
* True Negative indicates someone not having the disease and the model correctly predicting that they don't have the disease.
* False Positive indicates someone does not have the disease but the model predicts that they have the disease.
* False Negative indicates soemone does have the disease but the model also predicts that they don't have disease.  

False positive can be called as Type 1 error, whereas False Negative is called as Type 2 error.

## Evaluation Regression
Regression is a task when a model attempts to predict continuous values.
(For categorical, we use classification)

Most common evaluation metrics for regression are:
* Mean Absolute Error  -- mean of the absolute value of errors.
* Mean Squared Error  -- mean of the squared errors.
* Root Mean Square Error -- root of mean of the squared errors.

MAE won't punish large errors, there Mean Squared Error comes into play.
Large errors will be noted more with MAE.  
Units will also be squared with MSE like dollars square, which will be difficult to interpret, there root mean square error comes into play.

## Scikit Learn Package

Popular machine learning package with a lot of built in algorithms.

Install with anaconda distribution:
```
conda install scikit-learn
```
Install without anaconda ditribution:
```
pip install scikit-learn
```

Every alogorithm is exposed in scikit-learn via an 'Estimator'.
First we will import the model, the general form is:

`from sklearn.family import Model`

For Example:  
`frpm sklearn.linear_model import LinearRegression`

[Practice on Linear Regression](./10-Linear-Regression/LinearRegression.ipynb)

## Logistic Regression

The sigmoid or logistic function takes in any value and outputs it to be between 0 and 1.

### Confusion Matrix:
* True Positives (TP)  
  predicted - Yes  
  actual - Yes
* True Negatives (TN)  
  predicted - No  
  actual - No 
* False Positives (FP)  -- Type 1 Error  
  predicted - Yes  
  actual - No
* False Negatives (FN)  -- Type 2 Error  
  predicted - No  
  actual - Yes

Accuracy : (TP + TN) / Total  
MisClassification rate - Error rate (how often the model is wrong) : (FP + FN) / Total

[Practice on Logistic Regression](./10-Logistic-Regression/LogisticRegression.ipynb)


