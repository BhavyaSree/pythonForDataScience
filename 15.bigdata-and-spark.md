## Big Data

* Local process will use the computational resources of a single machine.
* A distributed process has access to the computational resources across a number of machines connected through a network.
* After a certain point, it is easier to scale ut to a many lower CPU machines, than to try to scale up to a single machine with high CPU.
* Distributed machines also have the advantage of easily scaling, you can just add more machines.
* They also include fault tolerance, if one machine fails, the whole network can still go on.

## Hadoop

* Hadoop is a way to distribute very large files across multiple machines.
* It uses the Hadoop Distributed File System (HDFS).
* HDFS allows a user to work with large data sets.
* HDFS also duplicates blocks of data for fault tolerance.
* It also then uses MapReduce
* MapReduce allows computations on that data

